# AI Blogger - Environment Configuration
# Copy this file to .env and fill in your API keys

# ============================================
# FREE LLM PROVIDERS (pick one or more)
# Priority order: cloudflare > groq > gemini > together > huggingface > ollama
# ============================================

# Cloudflare Workers AI - 10,000 free neurons/day
# Get free: https://dash.cloudflare.com/ -> AI -> Workers AI
CLOUDFLARE_ACCOUNT_ID=
CLOUDFLARE_API_TOKEN=

# Groq - RECOMMENDED (fastest, 30 req/min, 500k tokens/day)
# Get free API key: https://console.groq.com/keys
GROQ_API_KEY=

# Google Gemini - Very reliable (15 req/min, 1500 req/day)
# Get free API key: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=

# Together AI - Good alternative (free credits on signup)
# Get free API key: https://api.together.xyz/settings/api-keys
TOGETHER_API_KEY=

# Hugging Face - Fallback option (rate limited)
# Get free API key: https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=

# ============================================
# PROVIDER SELECTION
# ============================================
# Options: 'auto', 'cloudflare', 'groq', 'gemini', 'together', 'huggingface', 'ollama'
# 'auto' tries providers in order with smart rate limit rotation
LLM_PROVIDER=auto

# ============================================
# LOCAL OLLAMA (always works as fallback)
# ============================================
# If running Ollama locally, no API key needed
# Just ensure Ollama is running: ollama serve
OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_MODEL=llama3.1:latest

# ============================================
# SECURITY (auto-generated if not set)
# ============================================
# 32-character key for encrypting user API keys
# Generate with: openssl rand -hex 16
ENCRYPTION_KEY=

# ============================================
# DEPLOYMENT OPTIONS
# ============================================

# For Railway/Render (no local Ollama):
# 1. Set LLM_PROVIDER=auto or LLM_PROVIDER=groq
# 2. Add at least one cloud provider API key

# For Oracle Cloud Free Tier (with Ollama):
# 1. Set LLM_PROVIDER=ollama
# 2. Install Ollama on the VM and pull model
# 3. Set OLLAMA_URL=http://localhost:11434
